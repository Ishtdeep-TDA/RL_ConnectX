{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "025aa771-37f8-4eed-8071-7775802bac2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from simplified_connectx import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f3c17c-3ee4-4264-a2e0-af9d54108b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = SimplifiedConnectX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e2e215f-9c78-4752-9a22-14b5f2fac498",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (481565713.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    while resources_left(time, computational power):\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def monte_carlo_tree_search(root):\n",
    "     \n",
    "    while resources_left(time, computational power):\n",
    "        leaf = traverse(root)\n",
    "        simulation_result = rollout(leaf)\n",
    "        backpropagate(leaf, simulation_result)\n",
    "         \n",
    "    return best_child(root)\n",
    " \n",
    "    # function for node traversal\n",
    "    def traverse(node):\n",
    "        while fully_expanded(node):\n",
    "            node = best_uct(node)\n",
    "\n",
    "        # in case no children are present / node is terminal\n",
    "        return pick_unvisited(node.children) or node\n",
    "\n",
    "    # function for the result of the simulation\n",
    "    def rollout(node):\n",
    "        while non_terminal(node):\n",
    "            node = rollout_policy(node)\n",
    "        return result(node)\n",
    "\n",
    "    # function for randomly selecting a child node\n",
    "    def rollout_policy(node):\n",
    "        return pick_random(node.children)\n",
    "\n",
    "    # function for backpropagation\n",
    "    def backpropagate(node, result):\n",
    "        if is_root(node) return\n",
    "        node.stats = update_stats(node, result)\n",
    "        backpropagate(node.parent)\n",
    "\n",
    "    # function for selecting the best child\n",
    "    # node with highest number of visits\n",
    "    def best_child(node):\n",
    "        pick child with highest number of visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3a38b-15f9-4901-9a4e-7aaf291b72a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f871cc20-f77b-45b1-8495-5509ca421d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat GPT code\n",
    "import random\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, iterations):\n",
    "        self.game = game\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def search(self, state):\n",
    "        root = Node(state)\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            node = root\n",
    "            # Select a leaf node to expand\n",
    "            while not node.is_leaf():\n",
    "                node = self.select(node)\n",
    "\n",
    "            # Expand the leaf node and simulate a game from it\n",
    "            if not node.is_terminal():\n",
    "                node = self.expand(node)\n",
    "                reward = self.simulate(node.state)\n",
    "            else:\n",
    "                reward = node.state.utility()\n",
    "\n",
    "            # Backpropagate the reward up the tree\n",
    "            while node is not None:\n",
    "                node.update(reward)\n",
    "                node = node.parent\n",
    "\n",
    "        # Return the child with the highest number of visits\n",
    "        return self.select_best_child(root)\n",
    "\n",
    "    def select(self, node):\n",
    "        # Select the child with the highest UCB1 score\n",
    "        return max(node.children, key=lambda c: c.ucb1())\n",
    "\n",
    "    def expand(self, node):\n",
    "        # Add a random child to the node\n",
    "        child_state = random.choice(node.state.children())\n",
    "        child = Node(child_state, node)\n",
    "        node.add_child(child)\n",
    "        return child\n",
    "\n",
    "    def simulate(self, state):\n",
    "        # Play a random game from the current state and return the reward\n",
    "        while not state.is_terminal():\n",
    "            state = random.choice(state.children())\n",
    "        return state.utility()\n",
    "\n",
    "    def select_best_child(self, node):\n",
    "        # Return the child with the highest number of visits\n",
    "        return max(node.children, key=lambda c: c.visits)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, state, parent=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return self.state.is_terminal()\n",
    "\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def update(self, reward):\n",
    "        self.visits += 1\n",
    "        self.total_reward += reward\n",
    "\n",
    "    def ucb1(self):\n",
    "        # Calculate the UCB1 score for this node\n",
    "        if self.visits == 0:\n",
    "            return float('inf')\n",
    "\n",
    "        exploration_term = math.sqrt(2 * math.log(self.parent.visits) / self.visits)\n",
    "        return self.total_reward / self.visits + exploration_term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebdf7a9-eca5-4879-a4f6-c17e49aa8fe8",
   "metadata": {},
   "source": [
    "##### In MCTS we store 1 tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e51fe5dc-b2fe-48f4-91f0-9858f2a11fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_mcts():\n",
    "    '''\n",
    "    Implementation of MCTS algorithm based on the lecture 9\n",
    "    from David Silver's course on youtube.\n",
    "    \n",
    "    Before leaf node is found, we run UCB, and after that we run\n",
    "    the current policy.\n",
    "    \n",
    "    Leaf node is defined as a node which has not been seen by the UCB algorithm,\n",
    "    Each time a node is found which is not seen, it gets added into the UCB\n",
    "    tree (which is S_counts).\n",
    "    '''\n",
    "    def __init__(self,cur_state,nnet,args):\n",
    "        '''\n",
    "        state - It is the root state (in this case the game (which is a wrapper over\n",
    "        the simplifiedconnectx gym environment))\n",
    "        cur_policy - This is the policy which is run when the leaf node is found\n",
    "        args - this has arguments like exploration rate, num_mcts_sims\n",
    "        \n",
    "        '''\n",
    "        self.args = args\n",
    "        self.game = cur_state\n",
    "        #This is the dictionary which keeps track of the score for all states and actions\n",
    "        self.SA_score = {}\n",
    "        #This stores the state action pair counts\n",
    "        self.SA_counts = {}\n",
    "        # This is the dictionary which counts the number of times a state is visited\n",
    "        self.S_counts = {}\n",
    "        # This stores the total number of times a simulation has been run\n",
    "        self.total_counts = 0\n",
    "        # This is the exploration constant, which governs how much we should explore\n",
    "        # Higher c means higher exploration\n",
    "        self.c = args[\"c\"]\n",
    "        # This could be a NN or any policy. The input would be the board\n",
    "        # The output should be a probability distribution of moves\n",
    "        self.nnet = nnet\n",
    "        \n",
    "    def getActionProb(self,game,opt_player):\n",
    "        '''\n",
    "        This function runs all the simulations of the MCTS starting\n",
    "        from the root node and then returns the final prob distribution\n",
    "        \n",
    "        arguments - \n",
    "        game: this is the current state of the board\n",
    "        opt_player: this is the player to optimize for\n",
    "        '''\n",
    "        for i in range(self.args[\"num_mcts_sims\"]):\n",
    "            print(\"inside mcts, iteration number\",i)\n",
    "            self.search(game.create_copy(),opt_player)\n",
    "        \n",
    "        s = game.stringRepresentation()\n",
    "        # Lets get the scores of children of the current state s \n",
    "        prob = [self.SA_score[(s,a)] if (s,a) in self.SA_score else 0\\\n",
    "                for a in range(game.getActionSize())]\n",
    "        # Now we need to normalize these scores\n",
    "        total = sum(prob)\n",
    "        \n",
    "        if total == 0:\n",
    "            # should never reach here\n",
    "            print(\"There divide by zero in getActionProb\")\n",
    "        # Normalizing the probabilities\n",
    "        prob = [x/total for x in prob]\n",
    "        \n",
    "        # The value of being the state s is \n",
    "        # following the policy after all searches, what is the value\n",
    "        # Lets make this optimistic, which means that the current state is \n",
    "        # as good as the next best state and so\n",
    "        index_max = max(range(len(prob)), key=prob.__getitem__)\n",
    "        v = self.SA_score[(s,index_max)] / self.SA_counts[(s,index_max)]\n",
    "        \n",
    "        print(\"The probabilities for state S are :\", prob)\n",
    "        print(\"The value of being in state S: \", v)\n",
    "        \n",
    "        return prob,v\n",
    "        \n",
    "    def search(self,game,opt_player):\n",
    "        '''\n",
    "        This actually runs 1 episode from the total number of simulations\n",
    "        \n",
    "        If we are not at the leaf node, we should use UCB to pick which node to expand,\n",
    "        If we are at the leaf node, we should use the current policy to expand the tree\n",
    "        \n",
    "        '''\n",
    "        s = game.stringRepresentation()\n",
    "        \n",
    "        \n",
    "        #check if the game has ended\n",
    "        game_result = game.getGameEnded(opt_player)\n",
    "        if game_result != 0:\n",
    "            # The game has ended\n",
    "            return game_result\n",
    "        #check for leaf node\n",
    "        if s not in self.S_counts: # If this is true then this is a leaf node\n",
    "            # Now we will play a full game from here, using the NN and record the result\n",
    "            canonicalBoard = game.getCanonicalForm()\n",
    "            q_score,v_score = self.nnet.predict(canonicalBoard)\n",
    "            valids = game.getValidMoves()\n",
    "            q_score = q_score * valids  # masking invalid moves\n",
    "            sum_Ps_s = np.sum(q_score)\n",
    "            if sum_Ps_s > 0:\n",
    "                # q_score represents the prob distribution of our NN\n",
    "                q_score /= sum_Ps_s  # renormalize\n",
    "            else:\n",
    "                # if all valid moves were masked make all valid moves equally probable\n",
    "\n",
    "                # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n",
    "                log.error(\"All valid moves were masked, doing a workaround.\")\n",
    "                q_score = q_score + valids\n",
    "                q_score /= np.sum(q_score)\n",
    "            \n",
    "            # Lets choose a move from the prob distribution of this NN\n",
    "            action = np.random.choice(game.getActionSize(),p = q_score)\n",
    "            \n",
    "            nest_s, reward, done, info = game.getNextState(action)\n",
    "            result = self.search(game,opt_player)\n",
    "            \n",
    "            if result != 0:\n",
    "                self.SA_score[(s, action)] = result\n",
    "                self.SA_counts[(s, action)] = 1\n",
    "                self.S_counts[s] = 1\n",
    "                return result\n",
    "            \n",
    "        else: # not a leaf node\n",
    "            # We have to apply UCB as this has been visited before\n",
    "            best_action = None\n",
    "            best_action_score = -float(\"inf\")\n",
    "            valids = game.getValidMoves()\n",
    "            for action,move in enumerate(valids):\n",
    "                # action is valid\n",
    "                if move != 0:\n",
    "                    #check if we have visited this action\n",
    "                    if (s,action) in self.SA_score:\n",
    "                        UCB_score = self.SA_score[(s,action)] + self.c*(math.sqrt(self.total_counts))/(self.SA_counts[(s,action)])\n",
    "                    else:\n",
    "                        # This is max because self.SA_counts[(s,action)] = 0 which is the denominator\n",
    "                        UCB_score = float(\"inf\")\n",
    "                    if UCB_score > best_action_score:\n",
    "                        best_action_score = UCB_score\n",
    "                        best_action = action\n",
    "            \n",
    "            # Now that we have the best action, lets move it and continue\n",
    "            nest_s, reward, done, info = game.getNextState(best_action)\n",
    "            result = self.search(game,opt_player)\n",
    "            self.S_counts[s] += 1\n",
    "            # updating the score for the current state\n",
    "            if (s, best_action) in self.SA_score:\n",
    "                self.SA_score[(s, best_action)] = (self.SA_counts[(s, best_action)] * self.SA_score[(s, best_action)] + result) / (self.SA_counts[(s, best_action)] + 1)\n",
    "                self.SA_counts[(s, best_action)] += 1\n",
    "            else:\n",
    "                self.SA_score[(s, best_action)] = result\n",
    "                self.SA_counts[(s, best_action)] = 1\n",
    "            #passing the result to the parent\n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837190e-bae4-494a-830a-2a57e9639413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f63f9bd-f14d-4345-9efe-12d3da55fb46",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### MCTS Variant, instead of using UCB_score to be infinite and forcing the \n",
    "##### Algorithm to explore the whole space, we can \"guide\" it with NN predictions\n",
    "##### example p*(math.sqrt(self.total_counts))/(self.SA_counts[(s,action)])\n",
    "##### Where p is the probability of picking that action according to the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cf77c600-e9f9-43af-8104-ac72d3d89919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import coloredlogs\n",
    "\n",
    "from Coach import Coach\n",
    "from Game import Game\n",
    "from NeuralNet import NeuralNet as nn\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4b2d956-d655-4abb-bc84-ef1eab399719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "args = {\n",
    "    \"load_model\": False, # Whether to use a pretrained network \n",
    "    \"load_folder_file\":None, # \n",
    "    \"checkpoint\":\"./temp/\", # stores checkpoints here\n",
    "    \"num_mcts_sims\":20,\n",
    "    \"c\":0.1,\n",
    "    \"num_iters\":50, # number of times the NN is updated\n",
    "    \"maxlenOfQueue\": 200000,\n",
    "    \"numItersForTrainExamplesHistory\":20,\n",
    "    \"numEps\":20\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42c84cdf-1eef-4bf8-99ba-4ac0002dcf94",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:   0%|                                                                                                                                                                                                                                                  | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on move  1\n",
      "inside mcts, iteration number 0\n",
      "inside mcts, iteration number 1\n",
      "inside mcts, iteration number 2\n",
      "inside mcts, iteration number 3\n",
      "inside mcts, iteration number 4\n",
      "inside mcts, iteration number 5\n",
      "inside mcts, iteration number 6\n",
      "inside mcts, iteration number 7\n",
      "inside mcts, iteration number 8\n",
      "inside mcts, iteration number 9\n",
      "inside mcts, iteration number 10\n",
      "inside mcts, iteration number 11\n",
      "inside mcts, iteration number 12\n",
      "inside mcts, iteration number 13\n",
      "inside mcts, iteration number 14\n",
      "inside mcts, iteration number 15\n",
      "inside mcts, iteration number 16\n",
      "inside mcts, iteration number 17\n",
      "inside mcts, iteration number 18\n",
      "inside mcts, iteration number 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self Play:   0%|                                                                                                                                                                                                                                                  | 0/20 [00:07<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probabilities for state S are : [0.21666666666666667, 0.21666666666666667, 0.21666666666666667, -0.0, -0.08333333333333334, 0.21666666666666667, 0.21666666666666667]\n",
      "The value of being in state S:  -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m nnet \u001b[38;5;241m=\u001b[39m nn(g)\n\u001b[0;32m      4\u001b[0m c \u001b[38;5;241m=\u001b[39m Coach(g, nnet, args)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mG:\\Data Science\\Jupyter scripts\\COMP 552 RL\\Project\\Simplified connectX\\Coach.py:106\u001b[0m, in \u001b[0;36mCoach.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumEps\u001b[39m\u001b[38;5;124m\"\u001b[39m]), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelf Play\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmcts \u001b[38;5;241m=\u001b[39m my_mcts(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnnet, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs)  \u001b[38;5;66;03m# reset search tree\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     iterationTrainExamples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecuteEpisode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# save the iteration examples to the history \u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainExamplesHistory\u001b[38;5;241m.\u001b[39mappend(iterationTrainExamples)\n",
      "File \u001b[1;32mG:\\Data Science\\Jupyter scripts\\COMP 552 RL\\Project\\Simplified connectX\\Coach.py:78\u001b[0m, in \u001b[0;36mCoach.executeEpisode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b, p \u001b[38;5;129;01min\u001b[39;00m sym:\n\u001b[0;32m     76\u001b[0m     trainExamples\u001b[38;5;241m.\u001b[39mappend([b, opt_player, p, \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m---> 78\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m nest_s, reward, done, info \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mgetNextState(action)\n\u001b[0;32m     80\u001b[0m r \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mgetGameEnded(opt_player)\n",
      "File \u001b[1;32mmtrand.pyx:925\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Main cell to run the algorithm\n",
    "g = Game()\n",
    "nnet = nn(g)\n",
    "c = Coach(g, nnet, args)\n",
    "c.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1520227-a7fd-410f-a250-da3bc1f762a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab72bca-6f9a-43f7-b1d9-33042d1d82b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61866bed-f985-4de3-9b35-fbadc1473834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
